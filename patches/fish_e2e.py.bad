import base64
import ctypes
import io
import json
import os
import struct
from dataclasses import dataclass
from enum import Enum
from typing import AsyncGenerator, Union

import httpx
import numpy as np
import ormsgpack
import soundfile as sf
from loguru import logger

from tools.schema import (
    ServeMessage,
    ServeTextPart,
    ServeVQGANDecodeRequest,
    ServeVQGANEncodeRequest,
    ServeVQPart,
    ServeRequest,
)

logger.info("=============== FISH_E2E.PY LOADED ===============")
logger.info(f"Current file: {__file__}")
logger.info(f"Current directory: {os.getcwd()}")
logger.info(f"Directory contents: {os.listdir('.')}")
logger.info("================================================")

class CustomAudioFrame:
    def __init__(self, data, sample_rate, num_channels, samples_per_channel):
        if len(data) < num_channels * samples_per_channel * ctypes.sizeof(
            ctypes.c_int16
        ):
            raise ValueError(
                "data length must be >= num_channels * samples_per_channel * sizeof(int16)"
            )

        self._data = bytearray(data)
        self._sample_rate = sample_rate
        self._num_channels = num_channels
        self._samples_per_channel = samples_per_channel

    @property
    def data(self):
        return memoryview(self._data).cast("h")

    @property
    def sample_rate(self):
        return self._sample_rate

    @property
    def num_channels(self):
        return self._num_channels

    @property
    def samples_per_channel(self):
        return self._samples_per_channel


class FishE2EEventType(Enum):
    SPEECH_SEGMENT = 1
    TEXT_SEGMENT = 2
    END_OF_TEXT = 3
    END_OF_SPEECH = 4
    ASR_RESULT = 5
    USER_CODES = 6


@dataclass
class FishE2EEvent:
    type: FishE2EEventType
    frame: np.ndarray = None
    text: str = None
    vq_codes: list[list[int]] = None


client = httpx.AsyncClient(
    timeout=None,
    limits=httpx.Limits(
        max_keepalive_connections=5,
        max_connections=10,
        keepalive_expiry=5.0,
    ),
)


class FishE2EAgent:
    def __init__(self):
        self.llm_url = os.getenv("LLM_URL", "http://fish-agent:8080/v1/chat")
        self.vqgan_url = os.getenv("VQGAN_URL", "http://fish-agent:8080")
        self.client = httpx.AsyncClient(timeout=None)

    async def get_codes(self, audio_data, sample_rate):
        # Handle tuple format from Gradio (sample_rate, audio_data)
        if isinstance(audio_data, tuple):
            sample_rate, audio_data = audio_data

        logger.info(f"Converting audio data with shape: {audio_data.shape}")
        # Convert audio data to bytes
        audio_bytes = io.BytesIO()
        sf.write(audio_bytes, audio_data, sample_rate, format="WAV")
        audio_bytes.seek(0)

        # Send audio to VQGAN encoder
        payload = {"audios": [audio_bytes.read()]}
        logger.info("Sending request to VQGAN encoder...")
        resp = await self.client.post(
            f"{self.vqgan_url}/v1/vqgan/encode",
            content=ormsgpack.packb(payload),
            headers={"content-type": "application/msgpack"},
        )
        resp.raise_for_status()
        logger.info(f"Response content: {resp.content}")
        decoded = ormsgpack.unpackb(resp.content)
        logger.info(f"Decoded response: {decoded}")
        codes = decoded["tokens"][0][0]  # Get first token sequence since we only sent one audio
        return codes

    async def decode_send(self, codes):
        # Send codes to VQGAN decoder
        payload = {"codes": codes}
        resp = await self.client.post(
            f"{self.vqgan_url}/v1/vqgan/decode",
            content=ormsgpack.packb(payload),
            headers={"content-type": "application/msgpack"},
        )
        resp.raise_for_status()
        return ormsgpack.unpackb(resp.content)

    async def stream(
        self,
        system_audio_data: np.ndarray | None,
        user_audio_data: np.ndarray | None,
        sample_rate: int,
        num_channels: int,
        chat_ctx: dict | None = None,
    ) -> AsyncGenerator[bytes, None]:
        if system_audio_data is not None:
            sys_codes = await self.get_codes(system_audio_data, sample_rate)
        else:
            sys_codes = None

        if user_audio_data is not None:
            user_codes = await self.get_codes(user_audio_data, sample_rate)
            yield FishE2EEvent(
                type=FishE2EEventType.USER_CODES,
                vq_codes=user_codes,
            )
        else:
            user_codes = None

        # Step 2: Prepare LLM request
        if chat_ctx is None:
            sys_parts = [
                ServeTextPart(
                    text="You are a voice assistant created by Fish Audio, offering end-to-end voice interaction for a seamless user experience. You are required to first transcribe the user's speech, then answer it in the following format: 'Question: [USER_SPEECH]\n\nAnswer: [YOUR_RESPONSE]\n'. You are required to use the following voice in this conversation."
                ),
            ]
            if system_audio_data is not None:
                sys_parts.append(ServeVQPart(codes=[sys_codes]))
            chat_ctx = {
                "messages": [
                    ServeMessage(
                        role="system",
                        parts=sys_parts,
                    ).model_dump(),
                ],
            }

        # Step 3: Stream LLM response and decode audio
        request = ServeRequest(
            messages=chat_ctx["messages"]
            + (
                [
                    ServeMessage(
                        role="user",
                        parts=[ServeVQPart(codes=[user_codes])],
                    ).model_dump()
                ]
                if user_codes is not None and not chat_ctx["messages"]
                else []
            ),
            streaming=True,
            num_samples=1,
        )

        logger.info("Sending request to LLM...")
        logger.info(f"Request: {json.dumps(request.model_dump(), indent=2)}")

        # Send request
        resp = await self.client.post(
            self.llm_url,
            content=ormsgpack.packb(request, option=ormsgpack.OPT_SERIALIZE_PYDANTIC),
            headers={"content-type": "application/msgpack"},
        )
        resp.raise_for_status()

        buffer = b""
        vq_codes = []
        current_vq = False

        async def decode_send():
            nonlocal current_vq
            nonlocal vq_codes

            if not vq_codes:
                return

            # Decode VQ codes to audio
            decode_request = ServeVQGANDecodeRequest(tokens=vq_codes)
            logger.info(f"Decoding {len(vq_codes)} tokens...")
            decode_response = await self.client.post(
                f"{self.vqgan_url}/v1/vqgan/decode",
                content=ormsgpack.packb(decode_request),
                headers={"content-type": "application/msgpack"},
            )
            decode_response.raise_for_status()
            audio_data = ormsgpack.unpackb(decode_response.content)["audios"][0]
            yield FishE2EEvent(
                type=FishE2EEventType.SPEECH_SEGMENT,
                frame=np.frombuffer(audio_data, dtype=np.int16),
                vq_codes=vq_codes,
            )
            vq_codes.clear()
            current_vq = False

        async for chunk in resp.aiter_bytes():
            buffer += chunk
            while len(buffer) >= 4:
                read_length = struct.unpack("I", buffer[:4])[0]
                if len(buffer) < 4 + read_length:
                    break

                body = buffer[4 : 4 + read_length]
                buffer = buffer[4 + read_length :]
                data = ormsgpack.unpackb(body)
                logger.debug(f"Received data: {data}")

                if data["delta"] and data["delta"]["part"]:
                    if current_vq and data["delta"]["part"]["type"] == "text":
                        async for event in decode_send():
                            yield event
                    if data["delta"]["part"]["type"] == "text":
                        yield FishE2EEvent(
                            type=FishE2EEventType.TEXT_SEGMENT,
                            text=data["delta"]["part"]["text"],
                        )
                    elif data["delta"]["part"]["type"] == "vq":
                        vq_codes.extend(data["delta"]["part"]["codes"])
                        current_vq = True

        # Decode any remaining audio
        if vq_codes:
            async for event in decode_send():
                yield event


# Example usage:
async def main():
    import asyncio
    import soundfile as sf

    logger.info("Loading audio file test.wav...")
    # Load audio file
    audio_data, sample_rate = sf.read("test.wav")
    if len(audio_data.shape) == 1:
        audio_data = audio_data[:, None]

    # Create agent
    logger.info("Creating agent...")
    agent = FishE2EAgent()

    # Stream response
    logger.info("Starting stream...")
    async for event in agent.stream(
        system_audio_data=None,
        user_audio_data=audio_data,
        sample_rate=sample_rate,
        num_channels=audio_data.shape[1],
    ):
        if event.type == FishE2EEventType.TEXT_SEGMENT:
            print(f"Text: {event.text}")
        elif event.type == FishE2EEventType.SPEECH_SEGMENT:
            print(f"Speech frame shape: {event.frame.shape}")
        elif event.type == FishE2EEventType.ASR_RESULT:
            print(f"ASR: {event.text}")
        elif event.type == FishE2EEventType.END_OF_TEXT:
            print("End of text")
        elif event.type == FishE2EEventType.END_OF_SPEECH:
            print("End of speech")


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
